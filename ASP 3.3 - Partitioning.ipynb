{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b428fade-ea1e-417c-bdf4-92fd9befeb00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "111e3052-c864-4341-a17c-89d769f42016",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Partitioning\n",
    "##### Objectives\n",
    "1. Get partitions and cores\n",
    "1. Repartition DataFrames\n",
    "1. Configure default shuffle partitions\n",
    "\n",
    "##### Methods\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html\" target=\"_blank\">DataFrame</a>: `repartition`, `coalesce`, `rdd.getNumPartitions`\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkConf.html?#pyspark.SparkConf\" target=\"_blank\">SparkConf</a>: `get`, `set`\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#spark-session-apis\" target=\"_blank\">SparkSession</a>: `spark.sparkContext.defaultParallelism`\n",
    "\n",
    "##### SparkConf Parameters\n",
    "- `spark.sql.shuffle.partitions`, `spark.sql.adaptive.enabled`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5ae58b5-c647-471d-8b17-83e13d0cf52a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Finished setting up utiltity methods..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Finished setting up utiltity methods...",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Datasets mounted and student environment set up"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Datasets mounted and student environment set up",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ./Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "225aa647-147b-4694-9276-7051af48d616",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Get partitions and cores\n",
    "\n",
    "Use the `rdd` method `getNumPartitions` to get the number of DataFrame partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec3af7af-b40f-4f95-801a-6e26cdf7becf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: 4"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(eventsPath)\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7107cd91-0133-4c36-8c43-85f0f479b07c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Access `SparkContext` through `SparkSession` to get the number of cores or slots.\n",
    "\n",
    "Use the `defaultParallelism` attribute to get the number of cores in a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0a64f6c-dba0-498e-a5c3-0b24ca4ec9b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d15e27d-5835-40f7-b665-401871ba47c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`SparkContext` is also provided in Databricks notebooks as the variable `sc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91fc4030-38cd-4ccb-8dab-ff8237a7a41d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "836e005a-4da1-4669-ad03-d6db21c90108",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Repartition DataFrame\n",
    "\n",
    "There are two methods available to repartition a DataFrame: `repartition` and `coalesce`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "204b4392-ef4c-43d0-a48d-295bdd529a1b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### `repartition`\n",
    "Returns a new DataFrame that has exactly `n` partitions.\n",
    "\n",
    "- Wide transformation\n",
    "- Pro: Evenly balances partition sizes  \n",
    "- Con: Requires shuffling all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d60a8bb0-5b1b-475c-a4bf-c90d1bcd4f2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "repartitionedDF = df.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1585df3c-067c-4de3-8407-e98d360ce71e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: 8"
     ]
    }
   ],
   "source": [
    "repartitionedDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57b4de2f-35dd-49c2-a4bc-a41c23e2f8b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### `coalesce`\n",
    "Returns a new DataFrame that has exactly `n` partitions, when fewer partitions are requested.\n",
    "\n",
    "If a larger number of partitions is requested, it will stay at the current number of partitions.\n",
    "\n",
    "- Narrow transformation, some partitions are effectively concatenated\n",
    "- Pro: Requires no shuffling\n",
    "- Cons:\n",
    "  - Is not able to increase # partitions\n",
    "  - Can result in uneven partition sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c2686f8-cfcf-42ca-bf09-e9a9e1901a88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "coalesceDF = df.coalesce(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f34a8f18-d380-4e36-b8bb-04687b61b4f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: 4"
     ]
    }
   ],
   "source": [
    "coalesceDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e37a7bb-3cbe-4ac0-8b82-a68b23df9ec3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Configure default shuffle partitions\n",
    "\n",
    "Use the SparkSession's `conf` attribute to get and set dynamic Spark configuration properties. The `spark.sql.shuffle.partitions` property determines the number of partitions that result from a shuffle. Let's check its default value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "146a0191-6439-4c44-9efa-007da9d27aba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: '200'"
     ]
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0377dca-5d23-42d3-9655-47e059767506",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Assuming that the data set isn't too large, you could configure the default number of shuffle partitions to match the number of cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "017d0e65-aff0-4fd5-b7a4-1ccaa607f1e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd04e1c4-dd83-4a44-a0c1-5cc5d748f928",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Partitioning Guidelines\n",
    "- Make the number of partitions a multiple of the number of cores\n",
    "- Target a partition size of ~200MB\n",
    "- Size default shuffle partitions by dividing largest shuffle stage input by the target partition size (e.g., 4TB / 200MB = 20,000 shuffle partition count)\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> When writing a DataFrame to storage, the number of DataFrame partitions determines the number of data files written. (This assumes that <a href=\"https://sparkbyexamples.com/apache-hive/hive-partitions-explained-with-examples/\" target=\"_blank\">Hive partitioning</a> is not used for the data in storage. A discussion of DataFrame partitioning vs Hive partitioning is beyond the scope of this class.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43c7f29c-ca70-432a-a395-a922f12321b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Adaptive Query Execution\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/aspwd/partitioning_aqe.png\" width=\"60%\" />\n",
    "\n",
    "In Spark 3, <a href=\"https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution\" target=\"_blank\">AQE</a> is now able to <a href=\"https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html\" target=\"_blank\"> dynamically coalesce shuffle partitions</a> at runtime. This means that you can set `spark.sql.shuffle.partitions` based on the largest data set your application processes and allow AQE to reduce the number of partitions automatically when there is less data to process.\n",
    "\n",
    "The `spark.sql.adaptive.enabled` configuration option controls whether AQE is turned on/off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7cd9b6a-96e9-49d9-9180-56ea8ceed1cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: 'true'"
     ]
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.adaptive.enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d73db60-54c6-40d9-9c4a-266313634ae1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Clean up classroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f6d87b6-1097-477a-a6e4-316c8151d26e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "239829f8-63a8-432b-9e60-dea4f3c880f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ASP 3.3 - Partitioning",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
